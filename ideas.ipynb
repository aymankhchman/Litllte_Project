{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM+zgGK0tG/eX30YVijmhhN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aymankhchman/Litllte_Project/blob/master/ideas.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4yEjb1NECMvy"
      },
      "outputs": [],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "model_name = \"Qwen/Qwen2.5-7B-Instruct\"\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=\"auto\",\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "PpCYZmXVCqFi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def perplexity(text):\n",
        "  inputs = tokenizer(text, return_tensors='pt')\n",
        "  with torch.no_grad():\n",
        "    outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
        "    loss = outputs.loss\n",
        "  return torch.exp(loss).item(), outputs"
      ],
      "metadata": {
        "id": "OtQ_CKWq5Jcn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transcript = \"I need to buy something at the store.\"\n"
      ],
      "metadata": {
        "id": "wO9uaA7W6Wyn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "p, o = perplexity(transcript)"
      ],
      "metadata": {
        "id": "RAVZ8yf37BDz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dir(o)"
      ],
      "metadata": {
        "id": "lEbhyaF2HV9p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "o.logits"
      ],
      "metadata": {
        "id": "JSce4SM6HiVp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "BHfzkQ2sH-M9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "probs = F.softmax(o.logits, dim=-1)"
      ],
      "metadata": {
        "id": "TeGlAf42ICCs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "\n",
        "probs[0, torch.arange(probs.shape[1]), inputs[\"input_ids\"]]"
      ],
      "metadata": {
        "id": "z8_8bo56IV91"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(inputs[\"input_ids\"][0])):\n",
        "  id = inputs[\"input_ids\"][0][i].item()\n",
        "  print(tokenizer.convert_ids_to_tokens(id))"
      ],
      "metadata": {
        "id": "h9sc2Z9GIWAA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs[\"input\"]"
      ],
      "metadata": {
        "id": "wpTQVDgWIWB5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4SeXNDv-IWEC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "o.logits.shape"
      ],
      "metadata": {
        "id": "Ap7FXMBa7I-E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8xmBiD0LHVPd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = tokenizer('i need to buye sumthing at the stoure', return_tensors=\"pt\", return_offsets_mapping=True)"
      ],
      "metadata": {
        "id": "7qKlKWYd8KlL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs[\"input_ids\"][0][1]"
      ],
      "metadata": {
        "id": "5lrfVuxR9Gb0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs[\"offset_mapping\"]"
      ],
      "metadata": {
        "id": "NNMOZT6Y9OCk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.byte_encoder"
      ],
      "metadata": {
        "id": "5fl6RMea9QHh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = tokenizer.tokenize('i need to buye sumthing at the stoure.', return_tensors=\"pt\", return_offsets_mapping=True)"
      ],
      "metadata": {
        "id": "d0Newr3DMbzz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = tokenizer('i need to buye sumthing at the stoure.', return_tensors=\"pt\", return_offsets_mapping=True, add_special_tokens=False)"
      ],
      "metadata": {
        "id": "8kI6eQvCN4ZK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs[\"input_ids\"].shape"
      ],
      "metadata": {
        "id": "oei1z35IQJ-n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(tokens)"
      ],
      "metadata": {
        "id": "qOE0IqXjQKpV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids = inputs[\"input_ids\"]\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model(input_ids=input_ids, labels=input_ids)\n",
        "logits = outputs.logits  # Shape: (1, seq_len, vocab_size)"
      ],
      "metadata": {
        "id": "Gikl-tyQQSJJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logits.shape"
      ],
      "metadata": {
        "id": "N6EDUY-RXTIW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Shift logits and labels for alignment\n",
        "shift_logits = logits[..., :-1, :].contiguous()  # Predictions for tokens 1...n\n",
        "shift_labels = input_ids[..., 1:].contiguous()   # Ground truth for tokens 1...n"
      ],
      "metadata": {
        "id": "1XaHR_nYXUk0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "log_probs = torch.nn.functional.log_softmax(shift_logits, dim=-1)\n"
      ],
      "metadata": {
        "id": "mOZbKaUTXZ8y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "log_probs_for_labels = torch.gather(log_probs, -1, shift_labels.unsqueeze(-1)).squeeze(-1)\n",
        "probs = torch.exp(log_probs_for_labels).squeeze(0).tolist()"
      ],
      "metadata": {
        "id": "MsKpiI9bXh0O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "probs"
      ],
      "metadata": {
        "id": "ETtaD9VnXk2X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def group_tokens_into_words(tokens, probs):\n",
        "    word_probs = []\n",
        "    current_word = []\n",
        "    current_word_log_prob = 0\n",
        "\n",
        "    for token, prob in zip(tokens[1:], probs):  # Skip the first token (no context)\n",
        "        if token.startswith(\"Ġ\"):\n",
        "            # Save previous word\n",
        "            if current_word:\n",
        "                word_text = \"\".join(current_word)\n",
        "                word_prob = np.exp(current_word_log_prob)\n",
        "                word_probs.append((word_text, word_prob))\n",
        "            # Start new word\n",
        "            current_word = [token[1:]]  # Remove \"Ġ\"\n",
        "            current_word_log_prob = np.log(prob)\n",
        "        else:\n",
        "            # Append subword (e.g., \"thing\" to \"Ġsum\" → \"something\")\n",
        "            current_word.append(token)\n",
        "            current_word_log_prob += np.log(prob)\n",
        "\n",
        "    # Add the last word\n",
        "    if current_word:\n",
        "        word_text = \"\".join(current_word)\n",
        "        word_prob = np.exp(current_word_log_prob)\n",
        "        word_probs.append((word_text, word_prob))\n",
        "\n",
        "    return word_probs\n",
        "\n",
        "word_probs = group_tokens_into_words(tokens, probs)\n",
        "print(\"Word probabilities:\", word_probs)"
      ],
      "metadata": {
        "id": "ddd8cGMHXmS2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import RobertaTokenizer, RobertaForMaskedLM\n",
        "import torch\n",
        "\n",
        "model_name = \"roberta-large\"\n",
        "tokenizer = RobertaTokenizer.from_pretrained(model_name)\n",
        "model = RobertaForMaskedLM.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "t8s7mh4pX1Ub"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"I need to bye sumthing at the store.\"\n",
        "input_ids = tokenizer.encode(text, return_tensors=\"pt\")[0]\n",
        "tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
        "\n",
        "print(\"Tokens:\", tokens)\n",
        "# Example output: ['<s>', 'I', 'Ġneed', 'Ġto', 'Ġbye', 'Ġsum', 'thing', 'Ġat', 'Ġthe', 'Ġstore', '.', '</s>']"
      ],
      "metadata": {
        "id": "rPYKbGDjdBsj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens"
      ],
      "metadata": {
        "id": "iP0hNyHmdT-K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "logits = outputs.logits  # Shape: (1, seq_len, vocab_size)\n",
        "\n",
        "# Compute probabilities for all tokens\n",
        "probs = torch.softmax(logits, dim=-1).squeeze(0)  # Shape: (seq_len, vocab_size)\n",
        "token_probs = probs[torch.arange(len(input_ids)), input_ids].tolist()  # Probability of each token in context"
      ],
      "metadata": {
        "id": "uDJv_ADldDr_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "token_probs"
      ],
      "metadata": {
        "id": "VCkvzTFPdXNY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_token_probabilities(text, tokenizer, model):\n",
        "    input_ids = tokenizer.encode(text, return_tensors=\"pt\")[0]\n",
        "    seq_len = len(input_ids)\n",
        "    token_probs = []\n",
        "\n",
        "    for i in range(seq_len):\n",
        "        # Mask the i-th token\n",
        "        masked_input_ids = input_ids.clone()\n",
        "        masked_input_ids[i] = tokenizer.mask_token_id\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(masked_input_ids.unsqueeze(0))\n",
        "\n",
        "        # Get probabilities for the original token\n",
        "        probs = torch.softmax(outputs.logits[0, i], dim=-1)\n",
        "        original_token_id = input_ids[i].item()\n",
        "        token_prob = probs[original_token_id].item()\n",
        "        token_probs.append(token_prob)\n",
        "\n",
        "    return token_probs\n",
        "\n",
        "token_probs = compute_token_probabilities(text, tokenizer, model)\n",
        "print(\"Token probabilities:\", token_probs)"
      ],
      "metadata": {
        "id": "H73G2Ba3dF2f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def group_tokens_into_words(tokens, token_probs):\n",
        "    word_probs = []\n",
        "    current_word = []\n",
        "    current_word_log_prob = 0\n",
        "\n",
        "    for token, prob in zip(tokens, token_probs):\n",
        "        if token.startswith(\"Ġ\"):\n",
        "            # Save previous word\n",
        "            if current_word:\n",
        "                word_prob = np.exp(current_word_log_prob)\n",
        "                word_text = \"\".join(current_word)\n",
        "                word_probs.append((word_text, word_prob))\n",
        "            # Start new word\n",
        "            current_word = [token[1:]]  # Remove \"Ġ\"\n",
        "            current_word_log_prob = np.log(prob)\n",
        "        else:\n",
        "            # Append subword (e.g., \"thing\" to \"sum\")\n",
        "            current_word.append(token)\n",
        "            current_word_log_prob += np.log(prob)\n",
        "\n",
        "    # Add the last word\n",
        "    if current_word:\n",
        "        word_prob = np.exp(current_word_log_prob)\n",
        "        word_text = \"\".join(current_word)\n",
        "        word_probs.append((word_text, word_prob))\n",
        "\n",
        "    return word_probs\n",
        "\n",
        "word_probs = group_tokens_into_words(tokens, token_probs)\n",
        "low_prob_words = [word for word in word_probs if word[1] < 0.1]\n",
        "print(\"Low probability words:\", low_prob_words)"
      ],
      "metadata": {
        "id": "-NE7yUuWgVHC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_probs"
      ],
      "metadata": {
        "id": "7k1CDHr9dH9v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list(zip(tokens, token_probs))"
      ],
      "metadata": {
        "id": "jHP4fcksdLy0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import RobertaTokenizer, RobertaForMaskedLM, AutoModelForMaskedLM, AutoTokenizer\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "# Load RoBERTa\n",
        "model_name = \"roberta-large\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForMaskedLM.from_pretrained(model_name)\n",
        "\n",
        "text = \"I need to bye sumthing at the store.\"\n",
        "\n",
        "# Compute token probabilities (with special tokens handled)\n",
        "def compute_token_probabilities(text, tokenizer, model):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "    input_ids = inputs[\"input_ids\"][0]\n",
        "    tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
        "\n",
        "    # Remove special tokens\n",
        "    filtered_input_ids = input_ids[1:-1]\n",
        "    filtered_tokens = tokens[1:-1]\n",
        "    seq_len = len(filtered_input_ids)\n",
        "\n",
        "    token_probs = []\n",
        "    for i in range(seq_len):\n",
        "        masked_input_ids = filtered_input_ids.clone()\n",
        "        masked_input_ids[i] = tokenizer.mask_token_id\n",
        "\n",
        "        # Add back special tokens for proper processing\n",
        "        masked_input_ids = torch.cat([\n",
        "            torch.tensor([tokenizer.bos_token_id]),\n",
        "            masked_input_ids,\n",
        "            torch.tensor([tokenizer.eos_token_id])\n",
        "        ]).unsqueeze(0)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(masked_input_ids)\n",
        "\n",
        "        # Extract probability for the original token\n",
        "        probs = torch.softmax(outputs.logits[0, i+1], dim=-1)\n",
        "        token_prob = probs[filtered_input_ids[i]].item()\n",
        "        token_probs.append(token_prob)\n",
        "\n",
        "    return filtered_tokens, token_probs\n",
        "\n",
        "# Group tokens into words\n",
        "def group_tokens_into_words(tokens, token_probs):\n",
        "    word_probs = []\n",
        "    current_word = []\n",
        "    current_word_log_prob = 0\n",
        "\n",
        "    for token, prob in zip(tokens, token_probs):\n",
        "        if token.startswith(\"Ġ\"):\n",
        "            if current_word:\n",
        "                word_prob = np.exp(current_word_log_prob)\n",
        "                word_text = \"\".join(current_word)\n",
        "                word_probs.append((word_text, word_prob))\n",
        "            current_word = [token[1:]]\n",
        "            current_word_log_prob = np.log(prob)\n",
        "        else:\n",
        "            current_word.append(token)\n",
        "            current_word_log_prob += np.log(prob)\n",
        "\n",
        "    if current_word:\n",
        "        word_prob = np.exp(current_word_log_prob)\n",
        "        word_text = \"\".join(current_word)\n",
        "        word_probs.append((word_text, word_prob))\n",
        "\n",
        "    return word_probs\n",
        "\n",
        "# Flag low-probability words\n",
        "\n",
        "text = \"Aye am going to bye sum bread at the supermarket\"\n",
        "tokens, token_probs = compute_token_probabilities(text, tokenizer, model)\n",
        "word_probs = group_tokens_into_words(tokens, token_probs)\n",
        "low_prob_words = [word for word in word_probs if word[1] < 1e-4]\n",
        "print(\"Low probability words:\", low_prob_words)"
      ],
      "metadata": {
        "id": "uq0S4gUefUyN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_probs"
      ],
      "metadata": {
        "id": "LGXoeis1h1NV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "  tokens"
      ],
      "metadata": {
        "id": "XfEobSuoh8ll"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
        "\n",
        "model_name = \"FacebookAI/xlm-roberta-base\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForMaskedLM.from_pretrained(model_name)\n",
        "\n",
        "\n",
        "text = \"Aye am going to bye sum bread at the supermarket\"\n",
        "tokens = tokenizer.tokenize(text)\n",
        "print(\"Tokens:\", tokens)\n",
        "\n",
        "# Output: ['▁A', 'ye', '▁am', '▁going', '▁to', '▁by', 'e', '▁sum', '▁bread', '▁at', '▁the', '▁supermarket']\n",
        "\n",
        "def group_tokens_into_words(tokens, token_probs):\n",
        "    word_probs = []\n",
        "    current_word = []\n",
        "    current_word_log_prob = 0\n",
        "\n",
        "    for token, prob in zip(tokens, token_probs):\n",
        "        if token.startswith(\"▁\"):\n",
        "            # Save previous word\n",
        "            if current_word:\n",
        "                word_prob = np.exp(current_word_log_prob)\n",
        "                word_text = \"\".join(current_word)\n",
        "                word_probs.append((word_text, word_prob))\n",
        "            # Start new word\n",
        "            current_word = [token[1:]]  # Remove \"▁\"\n",
        "            current_word_log_prob = np.log(prob)\n",
        "        else:\n",
        "            # Append subword (e.g., \"ye\" to \"A\" → \"Aye\")\n",
        "            current_word.append(token)\n",
        "            current_word_log_prob += np.log(prob)\n",
        "\n",
        "    # Add the last word\n",
        "    if current_word:\n",
        "        word_prob = np.exp(current_word_log_prob)\n",
        "        word_text = \"\".join(current_word)\n",
        "        word_probs.append((word_text, word_prob))\n",
        "\n",
        "    return word_probs\n",
        "\n",
        "def compute_token_probabilities(text, tokenizer, model):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "    input_ids = inputs[\"input_ids\"][0]\n",
        "    tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
        "\n",
        "    # Remove special tokens ([CLS] and [SEP])\n",
        "    filtered_input_ids = input_ids[1:-1]\n",
        "    filtered_tokens = tokens[1:-1]\n",
        "    seq_len = len(filtered_input_ids)\n",
        "\n",
        "    token_probs = []\n",
        "    for i in range(seq_len):\n",
        "        masked_input_ids = filtered_input_ids.clone()\n",
        "        masked_input_ids[i] = tokenizer.mask_token_id\n",
        "\n",
        "        # Add back special tokens for proper processing\n",
        "        masked_input_ids = torch.cat([\n",
        "            torch.tensor([tokenizer.cls_token_id]),\n",
        "            masked_input_ids,\n",
        "            torch.tensor([tokenizer.sep_token_id])\n",
        "        ]).unsqueeze(0)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(masked_input_ids)\n",
        "\n",
        "        # Extract probability for the original token\n",
        "        probs = torch.softmax(outputs.logits[0, i+1], dim=-1)\n",
        "        token_prob = probs[filtered_input_ids[i]].item()\n",
        "        token_probs.append(token_prob)\n",
        "\n",
        "    return filtered_tokens, token_probs\n",
        "\n",
        "text = \"Aye am going to bye sum bread at the supermarket\"\n",
        "tokens, token_probs = compute_token_probabilities(text, tokenizer, model)\n",
        "word_probs = group_tokens_into_words(tokens, token_probs)\n",
        "low_prob_words = [word for word in word_probs if word[1] < 1e-4]\n",
        "print(\"Low probability words:\", low_prob_words)"
      ],
      "metadata": {
        "id": "2EUY8fwciR3T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_probs"
      ],
      "metadata": {
        "id": "HTaZMhwXlf5V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vzFgpRZllxey"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}